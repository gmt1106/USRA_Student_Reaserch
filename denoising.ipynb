{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-y53tpSG8Wg",
        "outputId": "c5c93c07-98e8-45b3-c1e5-572c55ab6da4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "*Uncomment if running on colab* \n",
        "Set Runtime -> Change runtime type -> Under Hardware Accelerator select GPU in Google Colab \n",
        "\"\"\"\n",
        "# !git clone https://github.com/ubc-vision/juho-usra.git\n",
        "# !mv juho-usra/* ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03wzm-DZI2PS",
        "outputId": "2bdeb2e6-1879-4190-ffe6-5f3c75828f6e"
      },
      "outputs": [],
      "source": [
        "# pip install pytorch_model_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGTmWVEchK2J",
        "outputId": "9f57d0b0-e5a1-48a7-821c-67f964760ff7"
      },
      "outputs": [],
      "source": [
        "# pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install lpips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWol3dXZG8Wl"
      },
      "source": [
        "# Import lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNGCiVmIG8Wo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from einops import rearrange\n",
        "import os\n",
        "from skimage.metrics import peak_signal_noise_ratio\n",
        "# from pytorch_model_summary import summary\n",
        "# use tensorboard with pytorch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from models import *\n",
        "# currently can't install this package \n",
        "# from siren_pytorch import SirenNet\n",
        "from models.siren_pytorch import SirenNet\n",
        "# import lpips\n",
        "\n",
        "dtype = None\n",
        "if torch.cuda.is_available():\n",
        "    dtype = torch.cuda.FloatTensor\n",
        "else:\n",
        "    dtype = torch.FloatTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW8Bso5MG8Wq"
      },
      "source": [
        "# Load image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "HmulR_SDG8Ws",
        "outputId": "b6dd93ec-45c6-41be-a918-475c44339aa0"
      },
      "outputs": [],
      "source": [
        "# std of the noise\n",
        "sigma = 25\n",
        "sigma_ = sigma/255.\n",
        "\n",
        "# set up original and noisy images for deep image prior \n",
        "path_to_image = 'data/denoising/F16_GT.png'\n",
        "img_orig_pil = Image.open(path_to_image)\n",
        "img_orig_np = np.array(img_orig_pil)\n",
        "img_orig_np = img_orig_np.transpose(2,0,1)\n",
        "img_orig_np = img_orig_np.astype(np.float32) / 255.\n",
        "# original \n",
        "# we usually need the dimensions to be divisible by a power of two (32 in this case)\n",
        "new_size = (img_orig_pil.size[0] - img_orig_pil.size[0] % 32, img_orig_pil.size[1] - img_orig_pil.size[1] % 32)\n",
        "bbox = [(img_orig_pil.size[0] - new_size[0])/2, \n",
        "        (img_orig_pil.size[1] - new_size[1])/2, \n",
        "        (img_orig_pil.size[0] + new_size[0])/2,\n",
        "        (img_orig_pil.size[1] + new_size[1])/2,]\n",
        "img_orig_pil = img_orig_pil.crop(bbox)\n",
        "img_orig_np = np.array(img_orig_pil)\n",
        "img_orig_np = img_orig_np.transpose(2,0,1)\n",
        "img_orig_np = img_orig_np.astype(np.float32) / 255.\n",
        "# noisy\n",
        "img_noisy_np = np.clip(img_orig_np + np.random.normal(scale=sigma, size=img_orig_np.shape), 0, 1).astype(np.float32)\n",
        "img_noisy_pil = np.clip(img_noisy_np*255,0,255).astype(np.uint8)\n",
        "img_noisy_pil = img_noisy_pil.transpose(1, 2, 0)\n",
        "img_noisy_pil = Image.fromarray(img_noisy_pil)\n",
        "\n",
        "net_input_width, net_input_height = img_orig_pil.size\n",
        "\n",
        "# show both original and noisy image\n",
        "plt.figure()\n",
        "plt.imshow(img_orig_pil)\n",
        "plt.figure()\n",
        "plt.imshow(img_noisy_pil)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhQwJiaoG8Wt"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA7fiz2fG8Wt",
        "outputId": "ec1fb42c-351e-4dcf-e827-5f240c5da334"
      },
      "outputs": [],
      "source": [
        "# Setup input meshgrid \n",
        "tensors = [torch.linspace(-1, 1, steps = net_input_height), torch.linspace(-1, 1, steps = net_input_width)]\n",
        "net_input = torch.stack(torch.meshgrid(*tensors), dim=-1).type(dtype)\n",
        "Deep_Image_Prior_net_input = rearrange(net_input, 'h w c -> () c h w', h = net_input_height, w = net_input_width).type(dtype).detach().requires_grad_()\n",
        "SIREN_net_input = rearrange(net_input, 'h w c -> (h w) c').type(dtype).detach().requires_grad_()\n",
        "input_depth = 2\n",
        "\n",
        "# Setup Deep Image Prior \n",
        "pad = 'reflection'\n",
        "\n",
        "deepImagePriorNet = DeepImagePriorNet (\n",
        "            input_depth, 3, \n",
        "            channels_down = [128, 128, 128, 128, 128],\n",
        "            channels_up = [128, 128, 128, 128, 128],\n",
        "            channels_skip = [4, 4, 4, 4, 4],\n",
        "            kernel_size_down = [3, 3, 3, 3, 3],\n",
        "            kernel_size_up = [3, 3, 3, 3, 3],\n",
        "            upsample_mode = 'bilinear',\n",
        "            need_sigmoid=True, need_bias=True, pad=pad)\n",
        "\n",
        "\n",
        "# Setup SIREN\n",
        "sirenNet = SirenNet(\n",
        "    dim_in = input_depth,              # input dimension, ex. 2d coor\n",
        "    dim_hidden = 256,                  # hidden dimension\n",
        "    dim_out = 3,                       # output dimension, ex. rgb value\n",
        "    num_layers = 5,                    # number of layers\n",
        "    w0_initial = 30.).type(dtype)      # different signals may require different omega_0 in the first layer - this is a hyperparameter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3aeoT8QG8Wv"
      },
      "source": [
        "# Deep Image Prior train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "4c0rlf22G8Wv",
        "outputId": "044c9796-13b4-4e9e-ec04-32be2acb47b1"
      },
      "outputs": [],
      "source": [
        "# Train for Deep Image Prior\n",
        "def deepImagePriorTrain(Deep_Image_Prior_net_input):\n",
        "\n",
        "    LR = 0.01\n",
        "    num_iter = 20000\n",
        "    reg_noise_std = 0.03\n",
        "    exp_weight = 0.99\n",
        "    Deep_Image_Prior_net_input_saved = Deep_Image_Prior_net_input.detach().clone()\n",
        "    noise = Deep_Image_Prior_net_input.detach().clone()\n",
        "    out_avg = None\n",
        "    last_net = None\n",
        "    psrn_noisy_last = 0\n",
        "    img_noisy = torch.from_numpy(img_noisy_np)[None, :].type(dtype)\n",
        "    img_orig = torch.from_numpy(img_orig_np)[None, :].type(dtype)\n",
        "\n",
        "    # Create optimizier\n",
        "    parameters = deepImagePriorNet.parameters()\n",
        "    optimizer = torch.optim.Adam(parameters, lr=LR)\n",
        "\n",
        "    # Loss\n",
        "    loss = nn.MSELoss().type(dtype) \n",
        "\n",
        "    # tensorboard log directory \n",
        "    log_dir = \"./logs/experiment/Deep_Image_Prior/denoising\"\n",
        "\n",
        "    # Create summary writer\n",
        "    writer = SummaryWriter(log_dir)\n",
        "\n",
        "    # Create log directory and save directory if it does not exist\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    # LPIPS evaluation\n",
        "    loss_fn_alex = lpips.LPIPS(net='alex').type(dtype) \n",
        "\n",
        "    # Training loop\n",
        "    for i in range(num_iter):\n",
        "\n",
        "        if reg_noise_std > 0:\n",
        "            Deep_Image_Prior_net_input = Deep_Image_Prior_net_input_saved + (noise.normal_() * reg_noise_std)\n",
        "\n",
        "        # Apply the model to obtain scores (forward pass)\n",
        "        out_orig = deepImagePriorNet.forward(Deep_Image_Prior_net_input)\n",
        "\n",
        "        # Smoothing \n",
        "        if out_avg is None:\n",
        "            out_avg = out_orig.detach()\n",
        "        else:\n",
        "            out_avg = out_avg * exp_weight + out_orig.detach() * (1 - exp_weight)\n",
        "\n",
        "        # Compute the loss        \n",
        "        total_loss = loss(out_orig, img_noisy)\n",
        "\n",
        "        # Compute gradients\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Zero the parameter gradients in the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Save the results\n",
        "        if i % 100 == 0:\n",
        "            # Write output image to tensorboard, using keywords `image_output`\n",
        "            writer.add_image(\"image_output\", out_orig, global_step=i, dataformats='NCHW')\n",
        "            # Write loss to tensorboard, using keywords `loss`\n",
        "            writer.add_scalar(\"loss\", total_loss, global_step=i)\n",
        "            # Write PSNR of noisy image \n",
        "            psrn_noisy = peak_signal_noise_ratio(img_noisy_np, out_orig.detach().cpu().numpy()[0]) \n",
        "            writer.add_scalar(\"noisy_img_PSNR\", psrn_noisy, global_step=i)\n",
        "            # Write PSNR of orig image\n",
        "            psrn_orig = peak_signal_noise_ratio(img_orig_np, out_orig.detach().cpu().numpy()[0])\n",
        "            writer.add_scalar(\"orig_img_PSNR\", psrn_orig, global_step=i)\n",
        "            # Write PSNR of orig image with overage of output\n",
        "            psrn_orig_sm = peak_signal_noise_ratio(img_orig_np, out_avg.detach().cpu().numpy()[0])\n",
        "            writer.add_scalar(\"orig_img_sm_PSNR\", psrn_orig_sm, global_step=i)\n",
        "            # Write LPIPS evaluation\n",
        "            lpips_evaluation = loss_fn_alex(img_orig, out_orig)\n",
        "            writer.add_scalar(\"LPIPS\", lpips_evaluation, global_step=i)\n",
        "\n",
        "            # Backtracking\n",
        "            if psrn_noisy - psrn_noisy_last < -5: \n",
        "                print('Falling back to previous checkpoint.')\n",
        "\n",
        "                for new_param, net_param in zip(last_net, deepImagePriorNet.parameters()):\n",
        "                    net_param.data.copy_(new_param.cuda())\n",
        "\n",
        "                return total_loss*0\n",
        "            else:\n",
        "                last_net = [x.detach().cpu() for x in deepImagePriorNet.parameters()]\n",
        "                psrn_noisy_last = psrn_noisy\n",
        "        \n",
        "\n",
        "deepImagePriorTrain(Deep_Image_Prior_net_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coX8qCm8G8Wx"
      },
      "source": [
        "# SIREN train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdpqP2hTG8Wy"
      },
      "outputs": [],
      "source": [
        "# Train for SIREN\n",
        "def sirenTrain(SIREN_net_input):\n",
        "\n",
        "    LR = 0.01\n",
        "    num_iter = 5000\n",
        "    exp_weight = 0.99\n",
        "    out_avg = None\n",
        "    img_noisy = torch.from_numpy(img_noisy_np)[None, :].type(dtype)\n",
        "    img_orig = torch.from_numpy(img_orig_np)[None, :].type(dtype)\n",
        "\n",
        "    # Create optimizier\n",
        "    parameters = sirenNet.parameters()\n",
        "    optimizer = torch.optim.Adam(parameters, lr=LR)\n",
        "\n",
        "    # Loss\n",
        "    loss = nn.MSELoss().type(dtype) \n",
        "\n",
        "    # tensorboard log directory \n",
        "    log_dir = \"./logs/experiment/Siren/denoising\"\n",
        "\n",
        "    # Create summary writer\n",
        "    writer = SummaryWriter(log_dir)\n",
        "\n",
        "    # Create log directory and save directory if it does not exist\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "    \n",
        "    # LPIPS evaluation\n",
        "    loss_fn_alex = lpips.LPIPS(net='alex').type(dtype)\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(num_iter):\n",
        "\n",
        "        # Apply the model to obtain scores (forward pass)\n",
        "        out_orig = sirenNet.forward(SIREN_net_input)\n",
        "        out_orig = rearrange(out_orig, '(h w) c -> () c h w', h = net_input_height, w = net_input_width)\n",
        "\n",
        "        # Smoothing \n",
        "        if out_avg is None:\n",
        "            out_avg = out_orig.detach()\n",
        "        else:\n",
        "            out_avg = out_avg * exp_weight + out_orig.detach() * (1 - exp_weight)\n",
        "\n",
        "        # Compute the loss \n",
        "        total_loss = loss(out_orig, img_noisy)\n",
        "\n",
        "        # Compute gradients    \n",
        "        total_loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Zero the parameter gradients in the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Save the results\n",
        "        if i % 25 == 0:\n",
        "             # Write output image to tensorboard, using keywords `image_output`\n",
        "            writer.add_image(\"image_output\", out_orig, global_step=i, dataformats='NCHW')\n",
        "            # Write loss to tensorboard, using keywords `loss`\n",
        "            writer.add_scalar(\"loss\", total_loss, global_step=i)\n",
        "            # Write PSNR of noisy image \n",
        "            psrn_noisy = peak_signal_noise_ratio(img_noisy_np, out_orig.detach().cpu().numpy()[0]) \n",
        "            writer.add_scalar(\"noisy_img_PSNR\", psrn_noisy, global_step=i)\n",
        "            # Write PSNR of orig image\n",
        "            psrn_orig = peak_signal_noise_ratio(img_orig_np, out_orig.detach().cpu().numpy()[0])\n",
        "            writer.add_scalar(\"orig_img_PSNR\", psrn_orig, global_step=i)\n",
        "            # Write PSNR of orig image with overage of output\n",
        "            psrn_orig_sm = peak_signal_noise_ratio(img_orig_np, out_avg.detach().cpu().numpy()[0])\n",
        "            writer.add_scalar(\"orig_img_sm_PSNR\", psrn_orig_sm, global_step=i)\n",
        "            # Write LPIPS evaluation\n",
        "            lpips_evaluation = loss_fn_alex(img_orig, out_orig)\n",
        "            writer.add_scalar(\"LPIPS\", lpips_evaluation, global_step=i)\n",
        "\n",
        "\n",
        "sirenTrain(SIREN_net_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Super Resolution.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "5cd7e1e7a3e2937747f4654778a100d513baa1a05dfa1f4c3bdb63bc7b1c1f34"
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('cv_project')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
